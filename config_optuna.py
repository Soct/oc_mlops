"""
Configuration pour l'optimisation des mod√®les avanc√©s.

Ce fichier permet de centraliser tous les param√®tres d'optimisation
sans avoir √† modifier le notebook principal.

Usage:
    from config_optuna import OPTUNA_CONFIG, MODEL_CONFIGS
"""

# ============================================================================
# CONFIGURATION G√âN√âRALE
# ============================================================================

OPTUNA_CONFIG = {
    # Nombre de trials par mod√®le
    "n_trials": {
        "quick": 20,      # Test rapide (~15-20 min total)
        "normal": 50,     # Optimisation normale (~45-60 min total)
        "deep": 100,      # Optimisation approfondie (~2-3h total)
        "extensive": 200  # Recherche extensive (~4-6h total)
    },
    
    # M√©trique √† optimiser
    "metric": "roc_auc",  # Options: 'roc_auc', 'f1', 'recall_minority', 'business_cost'
    
    # Validation crois√©e
    "cv_folds": 3,  # Nombre de folds (3-5 recommand√©)
    
    # Timeout (optionnel, en secondes)
    "timeout": None,  # Ex: 1800 pour 30 minutes max par mod√®le
    
    # Random seed pour reproductibilit√©
    "random_state": 42,
    
    # Sampler Optuna
    "sampler": "TPE",  # Options: 'TPE', 'Random', 'Grid', 'CmaEs'
    
    # Pruner (arr√™t pr√©coce des mauvais trials)
    "use_pruner": False,
    "pruner_config": {
        "n_startup_trials": 5,
        "n_warmup_steps": 10
    }
}


# ============================================================================
# ESPACES DE RECHERCHE PAR MOD√àLE
# ============================================================================

MODEL_CONFIGS = {
    
    # ========================================================================
    # XGBOOST
    # ========================================================================
    "xgboost": {
        "enabled": True,
        "search_space": {
            # Nombre d'arbres
            "n_estimators": {
                "type": "int",
                "low": 50,
                "high": 300,
                "step": 50
            },
            
            # Profondeur maximale
            "max_depth": {
                "type": "int",
                "low": 3,
                "high": 10
            },
            
            # Taux d'apprentissage
            "learning_rate": {
                "type": "float",
                "low": 0.01,
                "high": 0.3,
                "log": True
            },
            
            # √âchantillonnage des observations
            "subsample": {
                "type": "float",
                "low": 0.6,
                "high": 1.0
            },
            
            # √âchantillonnage des features
            "colsample_bytree": {
                "type": "float",
                "low": 0.6,
                "high": 1.0
            },
            
            # Poids minimum des enfants
            "min_child_weight": {
                "type": "int",
                "low": 1,
                "high": 10
            },
            
            # R√©duction minimale de perte pour split
            "gamma": {
                "type": "float",
                "low": 0,
                "high": 5
            },
            
            # R√©gularisation L1
            "reg_alpha": {
                "type": "float",
                "low": 0,
                "high": 10
            },
            
            # R√©gularisation L2
            "reg_lambda": {
                "type": "float",
                "low": 0,
                "high": 10
            }
        },
        
        # Param√®tres fixes (non optimis√©s)
        "fixed_params": {
            "random_state": 42,
            "n_jobs": -1,
            "use_label_encoder": False,
            "eval_metric": "logloss",
            "verbosity": 0
        }
    },
    
    # ========================================================================
    # LIGHTGBM
    # ========================================================================
    "lightgbm": {
        "enabled": True,
        "search_space": {
            # Nombre d'arbres
            "n_estimators": {
                "type": "int",
                "low": 50,
                "high": 300,
                "step": 50
            },
            
            # Profondeur maximale (-1 = illimit√©)
            "max_depth": {
                "type": "int",
                "low": 3,
                "high": 15
            },
            
            # Taux d'apprentissage
            "learning_rate": {
                "type": "float",
                "low": 0.01,
                "high": 0.3,
                "log": True
            },
            
            # Nombre de feuilles
            "num_leaves": {
                "type": "int",
                "low": 20,
                "high": 150
            },
            
            # √âchantillonnage des observations
            "subsample": {
                "type": "float",
                "low": 0.6,
                "high": 1.0
            },
            
            # √âchantillonnage des features
            "colsample_bytree": {
                "type": "float",
                "low": 0.6,
                "high": 1.0
            },
            
            # √âchantillons minimum par feuille
            "min_child_samples": {
                "type": "int",
                "low": 5,
                "high": 50
            },
            
            # R√©gularisation L1
            "reg_alpha": {
                "type": "float",
                "low": 0,
                "high": 10
            },
            
            # R√©gularisation L2
            "reg_lambda": {
                "type": "float",
                "low": 0,
                "high": 10
            }
        },
        
        # Param√®tres fixes
        "fixed_params": {
            "random_state": 42,
            "n_jobs": -1,
            "verbose": -1,
            "is_unbalance": True
        }
    },
    
    # ========================================================================
    # MLP (Multi-Layer Perceptron)
    # ========================================================================
    "mlp": {
        "enabled": True,
        "search_space": {
            # Nombre de couches cach√©es
            "n_layers": {
                "type": "int",
                "low": 1,
                "high": 3
            },
            
            # Neurones par couche (d√©fini dynamiquement)
            "n_units_range": {
                "type": "int",
                "low": 50,
                "high": 200,
                "step": 50
            },
            
            # Fonction d'activation
            "activation": {
                "type": "categorical",
                "choices": ["relu", "tanh"]
            },
            
            # R√©gularisation L2
            "alpha": {
                "type": "float",
                "low": 1e-5,
                "high": 1e-1,
                "log": True
            },
            
            # Taux d'apprentissage initial
            "learning_rate_init": {
                "type": "float",
                "low": 1e-4,
                "high": 1e-2,
                "log": True
            }
        },
        
        # Param√®tres fixes
        "fixed_params": {
            "solver": "adam",
            "max_iter": 300,
            "early_stopping": True,
            "validation_fraction": 0.1,
            "random_state": 42,
            "verbose": False
        }
    }
}


# ============================================================================
# CONFIGURATIONS PR√âD√âFINIES
# ============================================================================

PRESET_CONFIGS = {
    # Configuration pour un test rapide
    "quick_test": {
        "n_trials_mode": "quick",
        "cv_folds": 3,
        "models_to_run": ["lightgbm"],  # Le plus rapide
        "timeout": 600,  # 10 minutes max
    },
    
    # Configuration standard
    "standard": {
        "n_trials_mode": "normal",
        "cv_folds": 3,
        "models_to_run": ["xgboost", "lightgbm"],
        "timeout": None,
    },
    
    # Configuration approfondie
    "deep_search": {
        "n_trials_mode": "deep",
        "cv_folds": 5,
        "models_to_run": ["xgboost", "lightgbm", "mlp"],
        "timeout": None,
    },
    
    # Optimisation du co√ªt m√©tier
    "business_cost": {
        "n_trials_mode": "normal",
        "cv_folds": 3,
        "models_to_run": ["xgboost", "lightgbm"],
        "metric": "business_cost",
        "timeout": None,
    }
}


# ============================================================================
# M√âTRIQUES DISPONIBLES
# ============================================================================

METRICS_INFO = {
    "roc_auc": {
        "name": "ROC-AUC",
        "description": "Area Under the ROC Curve",
        "direction": "maximize",
        "range": [0, 1],
        "best_for": "Classification d√©s√©quilibr√©e g√©n√©rale"
    },
    
    "f1": {
        "name": "F1-Score",
        "description": "Harmonic mean of precision and recall",
        "direction": "maximize",
        "range": [0, 1],
        "best_for": "√âquilibre pr√©cision/recall"
    },
    
    "recall_minority": {
        "name": "Recall (classe 1)",
        "description": "True Positive Rate",
        "direction": "maximize",
        "range": [0, 1],
        "best_for": "Minimiser les faux n√©gatifs"
    },
    
    "business_cost": {
        "name": "Co√ªt m√©tier",
        "description": "FP √ó 1 + FN √ó 10 (n√©gatif)",
        "direction": "maximize",  # Moins n√©gatif = meilleur
        "range": [-float('inf'), 0],
        "best_for": "Optimisation selon co√ªts m√©tier r√©els"
    }
}


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def get_config(preset="standard"):
    """
    Retourne une configuration pr√©d√©finie.
    
    Args:
        preset: Nom du preset ('quick_test', 'standard', 'deep_search', 'business_cost')
    
    Returns:
        dict: Configuration compl√®te
    """
    if preset not in PRESET_CONFIGS:
        raise ValueError(f"Preset inconnu: {preset}. Choix: {list(PRESET_CONFIGS.keys())}")
    
    config = OPTUNA_CONFIG.copy()
    preset_config = PRESET_CONFIGS[preset]
    
    # Mettre √† jour avec les valeurs du preset
    config["n_trials"] = OPTUNA_CONFIG["n_trials"][preset_config["n_trials_mode"]]
    config["cv_folds"] = preset_config.get("cv_folds", 3)
    config["timeout"] = preset_config.get("timeout")
    config["metric"] = preset_config.get("metric", "roc_auc")
    config["models_to_run"] = preset_config.get("models_to_run", ["xgboost", "lightgbm", "mlp"])
    
    return config


def print_config(preset="standard"):
    """Affiche la configuration dans un format lisible."""
    config = get_config(preset)
    
    print(f"\n{'='*80}")
    print(f"Configuration: {preset.upper()}")
    print(f"{'='*80}\n")
    
    print(f"Optimisation:")
    print(f"  Trials par mod√®le: {config['n_trials']}")
    print(f"  M√©trique: {config['metric']}")
    print(f"  CV Folds: {config['cv_folds']}")
    print(f"  Timeout: {config['timeout'] or 'Aucun'}")
    print(f"  Sampler: {config['sampler']}")
    
    print(f"\nMod√®les activ√©s:")
    for model in config['models_to_run']:
        enabled = MODEL_CONFIGS[model]['enabled']
        status = "‚úÖ" if enabled else "‚ùå"
        print(f"  {status} {model.upper()}")
    
    print(f"\nTemps estim√©:")
    n_models = len(config['models_to_run'])
    min_time = (config['n_trials'] * 10 * n_models) // 60
    max_time = (config['n_trials'] * 30 * n_models) // 60
    print(f"  {min_time}-{max_time} minutes")
    
    print(f"\n{'='*80}\n")


# ============================================================================
# EXEMPLE D'UTILISATION
# ============================================================================

if __name__ == "__main__":
    # Afficher toutes les configurations disponibles
    print("üìã CONFIGURATIONS PR√âD√âFINIES\n")
    
    for preset_name in PRESET_CONFIGS.keys():
        print_config(preset_name)
    
    # Exemple : R√©cup√©rer une config
    config = get_config("standard")
    print(f"Nombre de trials: {config['n_trials']}")
    print(f"M√©trique: {config['metric']}")
